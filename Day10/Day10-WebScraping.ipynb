{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6d35f17b-9198-46d0-817e-73ef34eea956",
   "metadata": {},
   "source": [
    "---\n",
    "format:\n",
    "    html: \n",
    "        theme: minty\n",
    "        fontsize: 1.1em\n",
    "        page-layout: article\n",
    "        mermaid:\n",
    "            theme: default\n",
    "\n",
    "toc: true\n",
    "toc-depth: 5\n",
    "toc-expand: 2\n",
    "editor: visual\n",
    "warning: false\n",
    "error: false\n",
    "code-overflow: wrap\n",
    "eval: true\n",
    "\n",
    "title: \"Intermediate Data Science\"\n",
    "subtitle: \"Web Scraping\"\n",
    "author: \"Joanna Bieri <br> DATA201\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393c8ad4-4b58-43a8-91b7-f5c0ae5b7386",
   "metadata": {},
   "source": [
    "## Important Information\n",
    "\n",
    "- Email: [joanna_bieri@redlands.edu](mailto:joanna_bieri@redlands.edu)\n",
    "- Office Hours take place in Duke 209 -- [Office Hours Schedule](https://joannabieri.com/schedule.html)\n",
    "- [Class Website](https://joannabieri.com/data201.html)\n",
    "- [Syllabus](https://joannabieri.com/data201/IntermediateDataScience.pdf)\n",
    "\n",
    "## Web Scraping\n",
    "\n",
    "**Web scraping** is the process of automatically extracting data from websites using code. It often involves sending HTTP requests, retrieving HTML content, and parsing it to collect specific information such as text, images, or structured data. Web scraping can be useful for research, business intelligence, price comparison, academic projects, or building datasets that are not otherwise publicly available in a structured form.\n",
    "\n",
    "## Ethical Issues\n",
    "\n",
    "There are some things to be aware of before you start scraping data from the web. \n",
    "\n",
    "- Some data is private or protected. Just because you have access to a websites data doesn't mean you are allowed to scrape it. For example, when you log into Facebook or another social media site, you are granted special access to data about your connected people. It is unethical to use that access to scrape their private data!\n",
    "\n",
    "- Some websites have rules against scraping and will cut of service to users who are clearly scraping data. How do they know? Web scrapers access the website very differently that regular users. If they site has a policy about scraping data then you should follow it and/or content them about getting the data if you have a true academic interest in the data.\n",
    "\n",
    "- The line between web scraping and plagiarism can be very blurry. Make sure that you are citing where your data comes from AND not just reproducing the data exactly. Always citing the source of your data and make sure you are doing something new with it.\n",
    "\n",
    "- Ethics are different depending on if you are using the data for a personal project (eg. you just want to check scores for your favorite team daily and print the stuff you care about) vs if you are using the project for your business or website (eg. publishing information to drive clicks to your site/video/account or making money from the data you collect). In the later case it is EXTRA important to respect the original owner of the data. Drive web traffic back to their site, check with them about using their data, etc.\n",
    "\n",
    "The Ethical Scraper (from https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01):\n",
    "\n",
    "I, the web scraper will live by the following principles:\n",
    "\n",
    "- If you have a public API that provides the data I’m looking for, I’ll use it and avoid scraping all together.\n",
    "- I will always provide a User Agent string that makes my intentions clear and provides a way for you to contact me with questions or concerns.\n",
    "- I will request data at a reasonable rate. I will strive to never be confused for a DDoS attack.\n",
    "- I will only save the data I absolutely need from your page. If all I need it OpenGraph meta-data, that’s all I’ll keep.\n",
    "- I will respect any content I do keep. I’ll never pass it off as my own.\n",
    "- I will look for ways to return value to you. Maybe I can drive some (real) traffic to your site or credit you in an article or post.\n",
    "- I will respond in a timely fashion to your outreach and work with you towards a resolution.\n",
    "- I will scrape for the purpose of creating new value from the data, not to duplicate it.\n",
    "\n",
    "## Basics of HTML for Web Scraping\n",
    "\n",
    "Web scraping relies on understanding the structure of **HTML (HyperText Markup Language)**, since most web pages present their content using HTML tags. Each page is built from a tree-like structure called the **DOM (Document Object Model)**, which organizes elements hierarchically.\n",
    "\n",
    "## Key HTML Elements\n",
    "- **`<html>`**: The root element of an HTML page.  \n",
    "- **`<head>`**: Contains metadata (title, links to CSS/JS).  \n",
    "- **`<body>`**: Holds the visible content of the page.  \n",
    "\n",
    "## Common Tags\n",
    "- **Headings (`<h1>`, `<h2>`, ... `<h6>`)**: Define section titles.  \n",
    "- **Paragraph (`<p>`)**: Holds blocks of text.  \n",
    "- **Links (`<a href=\"...\">`)**: Anchor tags contain hyperlinks.  \n",
    "- **Images (`<img src=\"...\">`)**: Embed images via a source URL.  \n",
    "- **Lists (`<ul>`, `<ol>`, `<li>`)**: Ordered and unordered lists.  \n",
    "- **Tables (`<table>`, `<tr>`, `<td>`)**: Represent structured tabular data.  \n",
    "- **Divisions (`<div>`)**: Generic container often used for layout.  \n",
    "- **Spans (`<span>`)**: Inline container for styling or grouping.  \n",
    "\n",
    "## Attributes\n",
    "HTML tags often include **attributes** that provide additional information:\n",
    "- **`id`**: Unique identifier for an element.  \n",
    "- **`class`**: Groups elements with the same style or purpose.  \n",
    "- **`href`** (in `<a>`): Specifies the destination URL.  \n",
    "- **`src`** (in `<img>`): Provides the image source.  \n",
    "\n",
    "## Importance for Web Scraping\n",
    "- Scraping tools (like **BeautifulSoup**, **lxml**, or **Selenium**) locate elements by tags, attributes, or text.  \n",
    "- Common methods include selecting by `id`, `class`, or tag type.  \n",
    "- Understanding the **nested structure** of HTML helps identify where the target data resides in the DOM.  \n",
    "\n",
    "**Example Snippet:**\n",
    "```html\n",
    "<div class=\"product\">\n",
    "  <h2 class=\"title\">Book Title</h2>\n",
    "  <span class=\"price\">$19.99</span>\n",
    "</div>\n",
    "```\n",
    "## Challenges in Creating Web Scraping Code\n",
    "\n",
    "While web scraping is a powerful technique for gathering data, developers often face several significant challenges when implementing it:\n",
    "\n",
    "### 1. Changing Website Structures\n",
    "- Websites frequently update their HTML layout or CSS classes.\n",
    "- Even small changes (e.g., renaming a `class` or moving content into a new `<div>`) can break scraping scripts.\n",
    "- This requires ongoing maintenance and monitoring.\n",
    "\n",
    "### 2. JavaScript-Rendered Content\n",
    "- Many modern sites load data dynamically using JavaScript (AJAX requests, React, Angular, Vue).\n",
    "- Static HTML parsers (like BeautifulSoup) cannot capture this directly.\n",
    "- Developers may need to use tools like **Selenium**, **Playwright**, or network traffic analysis to retrieve dynamic content.\n",
    "\n",
    "### 3. Anti-Scraping Measures\n",
    "- Websites often deploy techniques to detect and block scrapers:\n",
    "  - CAPTCHAs\n",
    "  - Rate limiting\n",
    "  - IP blocking or blacklisting\n",
    "  - Bot detection systems\n",
    "- Workarounds may involve rotating proxies, adding delays, or mimicking real browser behavior.\n",
    "\n",
    "### 4. Data Quality and Cleaning\n",
    "- Extracted data is often messy (inconsistent formats, missing values, duplicates).\n",
    "- Developers must spend significant time cleaning, normalizing, and validating scraped data before analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf096b4a-cb9c-4f53-83e9-2a27e9604776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic package imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.defaule = 'colab'\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f247a1dd-199f-4044-bc8b-0724f58e8240",
   "metadata": {},
   "source": [
    "## Start with the URL\n",
    "\n",
    "The first thing you should inspect when scraping code is the website URL:\n",
    "\n",
    "    https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\n",
    "\n",
    "The URL has two major parts:\n",
    "\n",
    "- The base: https://realpython.github.io\n",
    "- The path: /fake-jobs/jobs/senior-python-developer-0.html\n",
    "\n",
    "The URL might also have query or pagination information included.\n",
    "\n",
    "### 1. Pagination in URLs\n",
    "Pagination often appears when content is split across multiple pages.\n",
    "\n",
    "**Examples:**\n",
    "- `https://example.com/products?page=1`\n",
    "- `https://example.com/products?page=2`\n",
    "- `https://example.com/articles?page=5&sort=latest`\n",
    "- `https://shop.example.com/category/shoes?p=3`\n",
    "\n",
    "Here, the parameter `page` (or sometimes `p`) changes to load different parts of the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Query Parameters in URLs\n",
    "Query parameters are key–value pairs that appear after a **`?`** in the URL. Multiple parameters are separated by **`&`**.\n",
    "\n",
    "**Examples:**\n",
    "- `https://example.com/search?q=laptop`\n",
    "- `https://example.com/search?q=laptop&sort=price_asc`\n",
    "- `https://example.com/api/items?category=books&limit=20&page=2`\n",
    "- `https://news.example.com/archive?year=2023&month=10`\n",
    "\n",
    "\n",
    "## Developer Tools\n",
    "\n",
    "The next thing you want to do is inspect the website with your browsers developer tools. Developer tools let you see the HTML code that the website uses. \n",
    "\n",
    "### Opening Developer Tools\n",
    "- **Mac:**  \n",
    "  - Press `Cmd + Option + I`  \n",
    "  - Or right-click on the page and choose **Inspect**\n",
    "- **Windows:**  \n",
    "  - Press `Ctrl + Shift + I` or `F12`  \n",
    "  - Or right-click on the page and choose **Inspect**\n",
    "- **Linux:**  \n",
    "  - Press `Ctrl + Shift + I` or `F12`  \n",
    "  - Or right-click on the page and choose **Inspect**\n",
    "\n",
    "---\n",
    "\n",
    "Try this! Navigate to https://realpython.github.io/fake-jobs/ and look at the code.\n",
    "\n",
    "Click on the drop down arrows to investigate the parts of the code. The browser should highlight the parts of the website that the code controls when you hover over the code. What do you notice about the organization of this site?\n",
    "\n",
    "## Scrape the HTML Content\n",
    "\n",
    "Install the requests package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0977fea-88c0-448d-93d8-bdf5de158321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge -y requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429fefbd-74fb-408b-9a96-921601e753c8",
   "metadata": {},
   "source": [
    "When we call `requests.get()` it grabs whatever information the website sends back. Sometimes this is really straightforward and sometimes you have to deal with issues like JavaScript code and CAPTCHA's.\n",
    "\n",
    "### Static vs. Dynamic Websites in Web Scraping\n",
    "\n",
    "When scraping websites, it’s important to know whether the site is **static** or **dynamic**, since this affects how data is loaded and how it can be extracted.\n",
    "\n",
    "\n",
    "### Static Websites\n",
    "- **Content delivery:** HTML content is fully loaded when the page is requested from the server.  \n",
    "- **Scraping approach:** The desired data is usually visible in the page’s source code and can be extracted with tools like **requests + BeautifulSoup**.  \n",
    "- **Example:** A blog where all posts are present in the HTML file.  \n",
    "- **Advantage:** Simple and predictable structure, easier to scrape.  \n",
    "\n",
    "\n",
    "### Dynamic Websites\n",
    "- **Content delivery:** The initial HTML may be minimal, and content is loaded later using JavaScript (e.g., AJAX, React, Angular).  \n",
    "- **Scraping approach:** The target data may not appear in the raw HTML source; instead it must be captured by:  \n",
    "  - Executing JavaScript (using **Selenium**, **Playwright**, or **Puppeteer**)  \n",
    "  - Inspecting the **Network panel** for API requests returning JSON data  \n",
    "- **Example:** An e-commerce site that loads more products as you scroll.  \n",
    "- **Challenge:** Requires more advanced tools and often more computing resources.  \n",
    "\n",
    "\n",
    "**In scraping practice:** Always check whether the content is present in the HTML source or if it only appears after interaction/JavaScript execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6e8f394-d9f4-4f6b-a49d-fd231fde61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "URL_dynamic = \"https://en.wikipedia.org/wiki/Cat\"\n",
    "\n",
    "page = requests.get(URL)\n",
    "\n",
    "# This prints out the whole website code for URL\n",
    "# print(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8615122b-5ef3-4e91-9055-19c202c3e1e7",
   "metadata": {},
   "source": [
    "## We will start with a Static Example\n",
    "\n",
    "## Parse the HTML code\n",
    "\n",
    "Now that you have the code as one big string in page.text you need a way to parse it. Beautiful Soup is a Python library for parsing structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62526fff-ca85-4e56-8e57-59eca054f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge -y beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad614e2e-74c0-472f-bfdd-a949050f63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Example HTML\n",
    "html = \"\"\"\n",
    "<div class=\"product\" id=\"p1\">\n",
    "  <h2 class=\"title\">Book One</h2>\n",
    "  <span class=\"price\">$19.99</span>\n",
    "  <a href=\"https://example.com/book1\" class=\"buy-link\">Buy</a>\n",
    "</div>\n",
    "<div class=\"product\" id=\"p2\">\n",
    "  <h2 class=\"title\">Book Two</h2>\n",
    "  <span class=\"price\">$24.99</span>\n",
    "  <a href=\"https://example.com/book2\" class=\"buy-link\">Buy</a>\n",
    "</div>\n",
    "<img src=\"cover1.jpg\" alt=\"Cover 1\">\n",
    "<img src=\"cover2.jpg\" alt=\"Cover 2\">\n",
    "\"\"\"\n",
    "\n",
    "# Parse HTML\n",
    "example_soup = BeautifulSoup(html, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e982e14b-9cd1-4762-bd09-17c4d7e66086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2 class=\"title\">Book One</h2>\n",
      "h2\n",
      "{'class': ['title']}\n",
      "Book One\n",
      "--------------------\n",
      "<h2 class=\"title\">Book Two</h2>\n",
      "h2\n",
      "{'class': ['title']}\n",
      "Book Two\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Find by tag name\n",
    "info = example_soup.find_all(\"h2\")\n",
    "for i in info:\n",
    "    print(i)\n",
    "    print(i.name)\n",
    "    print(i.attrs)\n",
    "    print(i.text)\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "576c834e-5df7-46da-b0d4-05170bf44ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"product\" id=\"p1\">\n",
      "<h2 class=\"title\">Book One</h2>\n",
      "<span class=\"price\">$19.99</span>\n",
      "<a class=\"buy-link\" href=\"https://example.com/book1\">Buy</a>\n",
      "</div>\n",
      "--------------------\n",
      "[<div class=\"product\" id=\"p1\">\n",
      "<h2 class=\"title\">Book One</h2>\n",
      "<span class=\"price\">$19.99</span>\n",
      "<a class=\"buy-link\" href=\"https://example.com/book1\">Buy</a>\n",
      "</div>, <div class=\"product\" id=\"p2\">\n",
      "<h2 class=\"title\">Book Two</h2>\n",
      "<span class=\"price\">$24.99</span>\n",
      "<a class=\"buy-link\" href=\"https://example.com/book2\">Buy</a>\n",
      "</div>]\n"
     ]
    }
   ],
   "source": [
    "# 2. Find by class\n",
    "first_product = example_soup.find(\"div\", class_=\"product\")\n",
    "print(first_product)\n",
    "print('--------------------')\n",
    "all_products = example_soup.find_all(\"div\", class_=\"product\")\n",
    "print(all_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e7fc014-1b78-43a3-910b-cce8d0287b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"product\" id=\"p1\">\n",
      "<h2 class=\"title\">Book One</h2>\n",
      "<span class=\"price\">$19.99</span>\n",
      "<a class=\"buy-link\" href=\"https://example.com/book1\">Buy</a>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "# 3. Find by id\n",
    "product_p1 = example_soup.find(\"div\", id=\"p1\")\n",
    "print(product_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f40d4a10-8264-42bb-8655-23256e7de680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"buy-link\" href=\"https://example.com/book1\">Buy</a>\n",
      "--------------------\n",
      "[<img alt=\"Cover 1\" src=\"cover1.jpg\"/>, <img alt=\"Cover 2\" src=\"cover2.jpg\"/>]\n"
     ]
    }
   ],
   "source": [
    "# 4. Find by other attributes\n",
    "first_link = example_soup.find(\"a\", href=\"https://example.com/book1\")\n",
    "print(first_link)\n",
    "print('--------------------')\n",
    "all_images = example_soup.find_all(\"img\", src=True)\n",
    "print(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3141f490-bbb6-472e-980d-4cd69b62b68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$19.99\n"
     ]
    }
   ],
   "source": [
    "# 5. Nested find\n",
    "product_price = first_product.find(\"span\", class_=\"price\").text\n",
    "print(product_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e4626-faa4-4a03-ae2f-5e4dd65777aa",
   "metadata": {},
   "source": [
    "**Using `.find()` and `.find_all()` in BeautifulSoup**\n",
    "\n",
    "BeautifulSoup provides two main methods for locating elements in an HTML document:\n",
    "\n",
    "- **`.find()`** → returns the **first matching element**.  \n",
    "- **`.find_all()`** → returns a **list of all matching elements**.\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80743aa8-c16a-405e-8de6-60b33a7bf441",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce123c03-3281-48ca-b1de-4cd1abfb25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find(id=\"ResultsContainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fff781b6-c56d-4579-96ce-e7532a675847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"card-content\">\n",
      " <div class=\"media\">\n",
      "  <div class=\"media-left\">\n",
      "   <figure class=\"image is-48x48\">\n",
      "    <img alt=\"Real Python Logo\" src=\"https://files.realpython.com/media/real-python-logo-thumbnail.7f0db70c2ed2.jpg?__no_cf_polish=1\"/>\n",
      "   </figure>\n",
      "  </div>\n",
      "  <div class=\"media-content\">\n",
      "   <h2 class=\"title is-5\">\n",
      "    Senior Python Developer\n",
      "   </h2>\n",
      "   <h3 class=\"subtitle is-6 company\">\n",
      "    Payne, Roberts and Davis\n",
      "   </h3>\n",
      "  </div>\n",
      " </div>\n",
      " <div class=\"content\">\n",
      "  <p class=\"location\">\n",
      "   Stewartbury, AA\n",
      "  </p>\n",
      "  <p class=\"is-small has-text-grey\">\n",
      "   <time datetime=\"2021-04-08\">\n",
      "    2021-04-08\n",
      "   </time>\n",
      "  </p>\n",
      " </div>\n",
      " <footer class=\"card-footer\">\n",
      "  <a class=\"card-footer-item\" href=\"https://www.realpython.com\" target=\"_blank\">\n",
      "   Learn\n",
      "  </a>\n",
      "  <a class=\"card-footer-item\" href=\"https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\" target=\"_blank\">\n",
      "   Apply\n",
      "  </a>\n",
      " </footer>\n",
      "</div>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job_cards = results.find_all(\"div\", class_=\"card-content\")\n",
    "# Lets look at the first job card\n",
    "print(job_cards[0].prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e73ada2-a883-4014-89a2-32e2843d1a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Python Developer</td>\n",
       "      <td>Payne, Roberts and Davis</td>\n",
       "      <td>Stewartbury, AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Energy engineer</td>\n",
       "      <td>Vasquez-Davidson</td>\n",
       "      <td>Christopherville, AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legal executive</td>\n",
       "      <td>Jackson, Chambers and Levy</td>\n",
       "      <td>Port Ericaburgh, AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>Savage-Bradley</td>\n",
       "      <td>East Seanview, AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product manager</td>\n",
       "      <td>Ramirez Inc</td>\n",
       "      <td>North Jamieview, AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Museum/gallery exhibitions officer</td>\n",
       "      <td>Nguyen, Yoder and Petty</td>\n",
       "      <td>Lake Abigail, AE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Radiographer, diagnostic</td>\n",
       "      <td>Holder LLC</td>\n",
       "      <td>Jacobshire, AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Database administrator</td>\n",
       "      <td>Yates-Ferguson</td>\n",
       "      <td>Port Susan, AE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Furniture designer</td>\n",
       "      <td>Ortega-Lawrence</td>\n",
       "      <td>North Tiffany, AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ship broker</td>\n",
       "      <td>Fuentes, Walls and Castro</td>\n",
       "      <td>Michelleville, AP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   job                     company  \\\n",
       "0              Senior Python Developer    Payne, Roberts and Davis   \n",
       "1                      Energy engineer            Vasquez-Davidson   \n",
       "2                      Legal executive  Jackson, Chambers and Levy   \n",
       "3               Fitness centre manager              Savage-Bradley   \n",
       "4                      Product manager                 Ramirez Inc   \n",
       "..                                 ...                         ...   \n",
       "95  Museum/gallery exhibitions officer     Nguyen, Yoder and Petty   \n",
       "96            Radiographer, diagnostic                  Holder LLC   \n",
       "97              Database administrator              Yates-Ferguson   \n",
       "98                  Furniture designer             Ortega-Lawrence   \n",
       "99                         Ship broker   Fuentes, Walls and Castro   \n",
       "\n",
       "                location  \n",
       "0        Stewartbury, AA  \n",
       "1   Christopherville, AA  \n",
       "2    Port Ericaburgh, AA  \n",
       "3      East Seanview, AP  \n",
       "4    North Jamieview, AP  \n",
       "..                   ...  \n",
       "95      Lake Abigail, AE  \n",
       "96        Jacobshire, AP  \n",
       "97        Port Susan, AE  \n",
       "98     North Tiffany, AA  \n",
       "99     Michelleville, AP  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = dict()\n",
    "\n",
    "for i,job_card in enumerate(job_cards):\n",
    "    # Get the information from each job card\n",
    "    title_element = job_card.find(\"h2\", class_=\"title\")\n",
    "    company_element = job_card.find(\"h3\", class_=\"company\")\n",
    "    location_element = job_card.find(\"p\", class_=\"location\")\n",
    "    # Add the information to the dictionarthy\n",
    "    jobs[i] = {'job':title_element.text.strip(),\n",
    "               'company':company_element.text.strip(),\n",
    "                'location':location_element.text.strip()}\n",
    "\n",
    "df = pd.DataFrame(jobs).T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1868a9-2033-44b6-b7a9-14ac3f9245f3",
   "metadata": {},
   "source": [
    "### Extracting Attributes\n",
    "\n",
    "We have already seen how to extract text from one of our elements. The text is just one part of the html code. Sometimes we want other parts of the content. For example, we may want to know about some of the attributes that come before the text. Attributes are not typed out to the website, but rather act on the text or help organize the elements on the page.\n",
    "\n",
    "### Common HTML Attributes\n",
    "\n",
    "HTML attributes provide additional information about elements and are often used in web scraping to locate or filter data. Some of the most common attributes include:\n",
    "\n",
    "- **id**  \n",
    "  - Uniquely identifies an element on the page.  \n",
    "  - Example: `<div id=\"main-content\">`\n",
    "\n",
    "- **class**  \n",
    "  - Groups elements for styling or scripting. Multiple elements can share the same class.  \n",
    "  - Example: `<span class=\"price\">`\n",
    "\n",
    "- **href**  \n",
    "  - Used in `<a>` tags to specify the link destination.  \n",
    "  - Example: `<a href=\"https://example.com\">Visit</a>`\n",
    "\n",
    "- **src**  \n",
    "  - Specifies the source for media elements like images, videos, or scripts.  \n",
    "  - Example: `<img src=\"cover.jpg\" alt=\"Book Cover\">`\n",
    "\n",
    "- **alt**  \n",
    "  - Provides alternative text for images, shown if the image cannot load.  \n",
    "  - Example: `<img src=\"cover.jpg\" alt=\"Book Cover\">`\n",
    "\n",
    "- **title**  \n",
    "  - Gives additional information about an element, usually shown as a tooltip.  \n",
    "  - Example: `<a href=\"#\" title=\"Click here for more info\">Link</a>`\n",
    "\n",
    "- **style**  \n",
    "  - Inline CSS styling for an element.  \n",
    "  - Example: `<p style=\"color:red;\">Important text</p>`\n",
    "\n",
    "- **name**  \n",
    "  - Often used in form elements to identify input fields.  \n",
    "  - Example: `<input type=\"text\" name=\"username\">`\n",
    "\n",
    "- **type**  \n",
    "  - Specifies the type of input or button in forms.  \n",
    "  - Example: `<input type=\"password\">`\n",
    "\n",
    "- **target**  \n",
    "  - Defines where to open linked documents (new tab, same tab, etc.).  \n",
    "  - Example: `<a href=\"https://example.com\" target=\"_blank\">Visit</a>`\n",
    "\n",
    "- **data-* (custom data attributes)**  \n",
    "  - Custom attributes used to store extra information on elements.  \n",
    "  - Example: `<div data-id=\"123\" data-category=\"books\">`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3654e466-171b-4aad-8dec-ccee6e3bce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2 class=\"title is-5\">Senior Python Developer</h2>\n"
     ]
    }
   ],
   "source": [
    "# Look at one\n",
    "job_card = job_cards[0]\n",
    "title_element = job_card.find(\"h2\", class_=\"title\")\n",
    "print(title_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4325dcd-77b3-4063-ac82-1bdafe957cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title', 'is-5']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get just the information about the class\n",
    "title_element['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c637618-9145-40e0-a9ff-e1557248c973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"buy-link\" href=\"https://example.com/book1\">Buy</a>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the example code above: get the first link\n",
    "link_example = example_soup.find('a')\n",
    "link_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19da3a16-11e1-44e9-851e-7a8252b481fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Buy'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the text\n",
    "link_example.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f943885-a65b-4104-badc-b515e8c11a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buy-link']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what class it belongs to\n",
    "link_example['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37b23146-8e9e-4aa7-be69-f8e7008a0288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://example.com/book1'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the url for the link\n",
    "link_example['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde42b6-44ab-470a-a77d-bcc616d90fba",
   "metadata": {},
   "source": [
    "## You Try\n",
    "\n",
    "Extract the application link for each of the jobs and add it to the data frame. Save this information to a list and then add it to the data frame as a column named \"application link\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b7aec93-dc42-4af3-88b7-15cbe7b094e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.realpython.com'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "links = []\n",
    "for i,job_card in enumerate(job_cards):\n",
    "    # Get the information from each job card\n",
    "    links.append(job_card.find(\"a\", class_=\"card-footer-item\"))\n",
    "\n",
    "links[0]['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d72a0-3744-44e2-8398-8f16a7fd282f",
   "metadata": {},
   "source": [
    "## More advanced Parsing\n",
    "\n",
    "### Functions and Beautiful Soup\n",
    "\n",
    "You can sometimes leverage functions inside the beautiful soup method to extract things in a more pythonic way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e52f5504-e0ca-450f-92ba-7a549edec977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2 class=\"title is-5\">Senior Python Developer</h2>,\n",
       " <h2 class=\"title is-5\">Software Engineer (Python)</h2>,\n",
       " <h2 class=\"title is-5\">Python Programmer (Entry-Level)</h2>,\n",
       " <h2 class=\"title is-5\">Python Programmer (Entry-Level)</h2>,\n",
       " <h2 class=\"title is-5\">Software Developer (Python)</h2>,\n",
       " <h2 class=\"title is-5\">Python Developer</h2>,\n",
       " <h2 class=\"title is-5\">Back-End Web Developer (Python, Django)</h2>,\n",
       " <h2 class=\"title is-5\">Back-End Web Developer (Python, Django)</h2>,\n",
       " <h2 class=\"title is-5\">Python Programmer (Entry-Level)</h2>,\n",
       " <h2 class=\"title is-5\">Software Developer (Python)</h2>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_jobs = results.find_all(\"h2\", \n",
    "                               string=lambda x: \"python\" in x.lower())\n",
    "\n",
    "python_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1de97874-4c91-4c32-aa4a-f10fcda4ea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senior Python Developer\n",
      "Software Engineer (Python)\n",
      "Python Programmer (Entry-Level)\n",
      "Python Programmer (Entry-Level)\n",
      "Software Developer (Python)\n",
      "Python Developer\n",
      "Back-End Web Developer (Python, Django)\n",
      "Back-End Web Developer (Python, Django)\n",
      "Python Programmer (Entry-Level)\n",
      "Software Developer (Python)\n"
     ]
    }
   ],
   "source": [
    "for job in python_jobs:\n",
    "    print(job.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1f7318a-a550-4f56-965e-a466fb74b51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2 class=\"title is-5\">Senior Python Developer</h2>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job = python_jobs[0]\n",
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6c30980-feae-4045-99d5-6d85f27503c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Why does this give an error?\u001b[39;00m\n\u001b[1;32m      2\u001b[0m title_element \u001b[38;5;241m=\u001b[39m job\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m title_element\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# Why does this give an error?\n",
    "title_element = job.find(\"h2\", class_=\"title\")\n",
    "title_element.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6298cf8b-a531-4d37-a540-6b71f619dce8",
   "metadata": {},
   "source": [
    "### Searching Hierarchy\n",
    "\n",
    "You can also search starting from an interior element. Here job is just one \"h2\" element, but we can look at its parent containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f8b4061-6042-418d-a417-924c20b1ece8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2 class=\"title is-5\">Senior Python Developer</h2>\n",
      "--------------------\n",
      "<div class=\"media-content\">\n",
      "<h2 class=\"title is-5\">Senior Python Developer</h2>\n",
      "<h3 class=\"subtitle is-6 company\">Payne, Roberts and Davis</h3>\n",
      "</div>\n",
      "--------------------\n",
      "<div class=\"media\">\n",
      "<div class=\"media-left\">\n",
      "<figure class=\"image is-48x48\">\n",
      "<img alt=\"Real Python Logo\" src=\"https://files.realpython.com/media/real-python-logo-thumbnail.7f0db70c2ed2.jpg?__no_cf_polish=1\"/>\n",
      "</figure>\n",
      "</div>\n",
      "<div class=\"media-content\">\n",
      "<h2 class=\"title is-5\">Senior Python Developer</h2>\n",
      "<h3 class=\"subtitle is-6 company\">Payne, Roberts and Davis</h3>\n",
      "</div>\n",
      "</div>\n",
      "--------------------\n",
      "<div class=\"card-content\">\n",
      "<div class=\"media\">\n",
      "<div class=\"media-left\">\n",
      "<figure class=\"image is-48x48\">\n",
      "<img alt=\"Real Python Logo\" src=\"https://files.realpython.com/media/real-python-logo-thumbnail.7f0db70c2ed2.jpg?__no_cf_polish=1\"/>\n",
      "</figure>\n",
      "</div>\n",
      "<div class=\"media-content\">\n",
      "<h2 class=\"title is-5\">Senior Python Developer</h2>\n",
      "<h3 class=\"subtitle is-6 company\">Payne, Roberts and Davis</h3>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"content\">\n",
      "<p class=\"location\">\n",
      "        Stewartbury, AA\n",
      "      </p>\n",
      "<p class=\"is-small has-text-grey\">\n",
      "<time datetime=\"2021-04-08\">2021-04-08</time>\n",
      "</p>\n",
      "</div>\n",
      "<footer class=\"card-footer\">\n",
      "<a class=\"card-footer-item\" href=\"https://www.realpython.com\" target=\"_blank\">Learn</a>\n",
      "<a class=\"card-footer-item\" href=\"https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\" target=\"_blank\">Apply</a>\n",
      "</footer>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "# We can look up through the hierarchy to get additional information\n",
    "print(job)\n",
    "print('--------------------')\n",
    "print(job.parent)\n",
    "print('--------------------')\n",
    "print(job.parent.parent)\n",
    "print('--------------------')\n",
    "print(job.parent.parent.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1a22e-02da-45ae-ba07-232fdc579a74",
   "metadata": {},
   "source": [
    "## You Try\n",
    "\n",
    "Try to scrape the quotes and authors from this website:\n",
    "\n",
    "https://quotes.toscrape.com/\n",
    "\n",
    "- what happens to the url when you push the next button at the bottom of the page? \n",
    "- what happens to the url when you click on a tag?\n",
    "\n",
    "Try to scrape all the quotes for one of the larger tags: love, inspirational, life, humor, or books. Make sure to get all the pages!\n",
    "\n",
    "Put this information into a DataFrame.\n",
    "\n",
    "Challenge (optional) - Scrape all the quotes on the site along with authors names and tags for each quote. Put all of this into a DataFrame and do an analysis. Who has the most quotes, longest or shortest quotes, most love quotes, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf42994-64dc-4e23-8a4a-025908f37e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91645c7c-1136-4f6a-9fdf-95967419e631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268bd174-6894-488e-8731-72c4494090ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e9e5940-7f09-4e95-9c74-5c8492773201",
   "metadata": {},
   "source": [
    "## Dynamic Web Scraping Process Using Selenium\n",
    "\n",
    "Dynamic websites load content using JavaScript, or other dynamic processes. Unlike static pages, the HTML source may initially contain little data, requiring tools like **Selenium** to interact with the page and extract information.\n",
    "\n",
    "Prepare the driver/browser\n",
    "\n",
    "1. We need to install selenium and webdriver-manager\n",
    "2. Then we import all the packages - my example is using chrome\n",
    "3. Choose options that work for your computer\n",
    "\n",
    "Now we are ready to scrape\n",
    "\n",
    "1. Start the driver\n",
    "2. Get the html - waiting for the JavaScript to load\n",
    "3. Execute the script to get the HTML\n",
    "4. Use Beautiful Soup to parse the HTML\n",
    "5. Close the browser\n",
    "\n",
    "**Key Benefit:** Selenium allows scraping of content that is not present in the initial HTML source, making it ideal for modern, JavaScript-heavy websites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7ae0caff-bda0-453c-bf33-3aa1351e0a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Cat'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try our Dynamic site\n",
    "URL_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f107a042-4073-44a3-bfa7-ca1e636cdc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge -y selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "116ef570-7a09-4f66-a356-819a313f2853",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now we need to import a lot of packages!\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# I am using chrome here - you need to already have chrome installed\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Service\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Options\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "# Now we need to import a lot of packages!\n",
    "# I am using chrome here - you need to already have chrome installed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48482164-8549-4ffa-b36e-52e89f8d31ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Options' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Configure Chrome options\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# These are the options that I needed for my computer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# I would keep --headless or --headless=new\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# The others you could probably comment out!\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m options \u001b[38;5;241m=\u001b[39m Options()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#options.add_argument(\"--headless=new\") # So a browser window doesnt open\u001b[39;00m\n\u001b[1;32m      7\u001b[0m options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--no-sandbox\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Options' is not defined"
     ]
    }
   ],
   "source": [
    "# Configure Chrome options\n",
    "# These are the options that I needed for my computer\n",
    "# I would keep --headless or --headless=new\n",
    "# The others you could probably comment out!\n",
    "options = Options()\n",
    "#options.add_argument(\"--headless=new\") # So a browser window doesnt open\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--remote-debugging-port=9222\")\n",
    "\n",
    "# Annoyingly I had to specify an exact driver version\n",
    "# Driver manager should figure this out for me, but it didn't\n",
    "# You should be able to just run: service = Service(ChromeDriverManager().install())\n",
    "service = Service(ChromeDriverManager(driver_version=\"140.0.7339.185\").install())\n",
    "driver = webdriver.Chrome(service=service,options=options)\n",
    "\n",
    "# Use the driver to get the webpage\n",
    "driver.get(URL_dynamic)\n",
    "\n",
    "# Wait for JavaScript to load\n",
    "#time.sleep(3)\n",
    "\n",
    "# Extract content (stuff on the page)\n",
    "html_source_code = driver.execute_script(\"return document.body.innerHTML;\")\n",
    "# Save to beautiful soup\n",
    "soup = BeautifulSoup(html_source_code, 'html.parser')\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe556b64-cbcf-4e5c-a4bb-13eee1ea6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can extract the information\n",
    "cats = soup.find_all('div', class_=\"tsingle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b1da806-84b7-4a80-9af4-77509cf086f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I really need to see some cute cat photos!\n",
    "for cat in cats:\n",
    "    photo = cat.find('a')\n",
    "    print('https://en.wikipedia.org/'+photo['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22dda5e-13ff-41f4-9b5c-1f7a5a485e3a",
   "metadata": {},
   "source": [
    "## Practice! Practice! Practice!\n",
    "\n",
    "The more you practice web scraping the better you will get at dealing with the curveballs that get thrown your way. Here are some great places to try out your skills\n",
    "\n",
    "https://www.scrapethissite.com/pages/\n",
    "\n",
    "https://webscraper.io/test-sites\n",
    "\n",
    "https://realpython.github.io/fake-jobs/\n",
    "\n",
    "https://books.toscrape.com/\n",
    "\n",
    "https://quotes.toscrape.com/\n",
    "\n",
    "https://www.wikipedia.org/ - NOT A FAKE SITE - but good for practice. Please follow their rules for scraping: https://wikitech.wikimedia.org/wiki/Robot_policy, basically this means don't send too many requests, if you are looking for large amounts of data use the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f00063-4ce6-49f3-85ac-e5a09fb32499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
